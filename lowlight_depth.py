#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
lowlight_depth.py
After Zero-DCE++ inference, utilize depth generated by MiDaS (optional confidence) to perform
"depth-adaptive denoising + fallback blending",
and optionally "intensity gating (depth controls brightness enhancement magnitude)",
to achieve cleaner distant/noisy regions and clearer foreground subjects.

Usage Example (Recommended):
  python lowlight_depth.py \
      --input data/test_data \
      --output result_Zero_DCE++_depth \
      --weights snapshots_Zero_DCE++/Epoch99.pth \
      --depth_dir midas_depth --denoise_far --use_conf \
      --far_gamma 2.0 --intensity_gate --gamma_i 2.0 \
      --d_pow 0.7 --d_p1 5 --d_p2 95 \
      --device 0

Standard Inference Only (same as original):
  python lowlight_depth.py --input data/test_data --output result_Zero_DCE++ --weights snapshots_Zero_DCE++/Epoch99.pth
"""

import os, glob, argparse, time
from pathlib import Path
import numpy as np
import cv2 as cv
from PIL import Image

import torch
import torch.nn as nn
import torchvision
import torch.backends.cudnn as cudnn

import model  # Your Zero-DCE++ model definition


# -------------------- Utility Functions --------------------
def _norm01(x, p1=1, p2=99, eps=1e-6):
    lo, hi = np.percentile(x, p1), np.percentile(x, p2)
    if hi - lo < eps:
        return np.zeros_like(x, dtype=np.float32)
    return np.clip((x - lo) / (hi - lo + eps), 0, 1).astype(np.float32)

def _load_depth_for_stem(depth_dir: Path, stem: str, d_p1=1, d_p2=99, d_pow=1.0):
    """Priority *_Dnorm.npy -> *_Dnorm.png -> *_disp.npy -> *_disp.png; and apply percentile stretch + power stretch"""
    cand = [f"{stem}_Dnorm.npy", f"{stem}_Dnorm.png", f"{stem}_disp.npy", f"{stem}_disp.png"]
    d = None
    for name in cand:
        p = depth_dir / name
        if not p.exists():
            continue
        if p.suffix == ".npy":
            d = np.load(p).astype(np.float32)
            d = np.clip(d, 0, 1) if "Dnorm" in name else _norm01(d, d_p1, d_p2)
            break
        im = cv.imread(str(p), cv.IMREAD_UNCHANGED)
        if im is None:
            continue
        if im.ndim == 3:
            im = cv.cvtColor(im, cv.COLOR_BGR2GRAY)
        im = im.astype(np.float32)
        if im.max() > 1:
            im /= 255.0
        d = np.clip(im, 0, 1) if "Dnorm" in name else _norm01(im, d_p1, d_p2)
        break
    if d is None:
        return None
    # Power stretch (boost near view, expand dynamic range)
    if d_pow != 1.0:
        d = np.clip(d, 0, 1) ** float(d_pow)
    return np.clip(d, 0, 1).astype(np.float32)

def _load_conf_for_stem(depth_dir: Path, stem: str):
    p = depth_dir / f"{stem}_conf.npy"
    return np.load(p).astype(np.float32) if p.exists() else None

def _prep_weight_far(D, conf=None, target_hw=None, gamma=1.0):
    """
    D∈[0,1] Near=1 Far=0 -> Far weight w_far = (1-D)^gamma; add bilateral filter/confidence; can resize to target size
    """
    D = np.clip(D.astype(np.float32), 0, 1)
    # Edge-preserving smoothing, reduce color banding/blocky noise
    D = cv.bilateralFilter(D, d=0, sigmaColor=25, sigmaSpace=12)
    w_far = (1.0 - D) ** float(gamma)
    if conf is not None:
        conf = np.clip(conf.astype(np.float32), 0, 1)
        w_far *= (0.3 + 0.7 * conf)  # Low-confidence regions have less impact
    if target_hw is not None and (w_far.shape != target_hw):
        h, w = target_hw
        w_far = cv.resize(w_far, (w, h), interpolation=cv.INTER_CUBIC)
    # Light smoothing to avoid checkerboard patterns
    w_far = cv.GaussianBlur(w_far, (0,0), 1.0)
    return np.clip(w_far, 0, 1).astype(np.float32)  # [H,W]

def _ultra_denoise_rgb01(img_rgb01: np.ndarray, w_far01: np.ndarray,
                         strong=(15, 15, 7, 21), light=(0, 0, 7, 21)):
    """
    Ultra-strong denoising: prioritize chroma noise -> then luma; use two-level NLM + Y/Cr/Cb separation + multi-scale strategy; blend using w_far
    """
    img_rgb01 = np.clip(img_rgb01, 0, 1).astype(np.float32)
    H, W = img_rgb01.shape[:2]

    # --- Chroma noise first: Strong smoothing on chroma channels (Cr/Cb) in YCrCb space
    ycrcb = cv.cvtColor((img_rgb01*255).astype(np.uint8), cv.COLOR_RGB2YCrCb).astype(np.float32)
    Y  = ycrcb[:,:,0] / 255.0
    Cr = ycrcb[:,:,1]
    Cb = ycrcb[:,:,2]

    Cr_s = cv.fastNlMeansDenoising(Cr.astype(np.uint8), None, 7, 7, 21).astype(np.float32)
    Cb_s = cv.fastNlMeansDenoising(Cb.astype(np.uint8), None, 7, 7, 21).astype(np.float32)
    Cr_l = cv.bilateralFilter(Cr, d=0, sigmaColor=25, sigmaSpace=12)
    Cb_l = cv.bilateralFilter(Cb, d=0, sigmaColor=25, sigmaSpace=12)
    Cr_f = (w_far01 * Cr_s + (1.0 - w_far01) * Cr_l)
    Cb_f = (w_far01 * Cb_s + (1.0 - w_far01) * Cb_l)

    # --- Luma denoising: Two-level NLM + bilateral, blend using w_far
    Y8 = (np.clip(Y,0,1)*255).astype(np.uint8)
    Y_dn_strong = cv.fastNlMeansDenoising(Y8, None, strong[0], strong[2], strong[3]).astype(np.float32)/255.0
    Y_dn_light  = cv.fastNlMeansDenoising(Y8, None, light[0],  light[2],  light[3]).astype(np.float32)/255.0
    Y_bi        = cv.bilateralFilter(Y.astype(np.float32), d=0, sigmaColor=12, sigmaSpace=7)
    Y_mix = (w_far01 * np.maximum(Y_dn_strong, Y_bi)) + ((1.0 - w_far01) * np.maximum(Y_dn_light, Y_bi))

    # --- Multi-scale remedy: Downsample, denoise, then upsample (suppress large-grain noise)
    Y_small = cv.resize(Y, (W//2, H//2), interpolation=cv.INTER_AREA)
    Y_small_dn = cv.fastNlMeansDenoising((Y_small*255).astype(np.uint8), None, 10, 7, 21).astype(np.float32)/255.0
    Y_ms = cv.resize(Y_small_dn, (W, H), interpolation=cv.INTER_CUBIC)
    Y_mix = 0.7 * Y_mix + 0.3 * Y_ms

    # --- Composite YCrCb -> RGB
    ycrcb_f = np.stack([(np.clip(Y_mix,0,1)*255),
                        np.clip(Cr_f,0,255),
                        np.clip(Cb_f,0,255)], axis=-1).astype(np.uint8)
    rgb_dn = cv.cvtColor(ycrcb_f, cv.COLOR_YCrCb2RGB).astype(np.float32)/255.0
    return np.clip(rgb_dn, 0, 1).astype(np.float32)

def depth_aware_denoise_rgb(enh_rgb01: np.ndarray,
                            w_far01: np.ndarray,
                            strong=(15, 15, 7, 21),
                            light=(0, 0, 7, 21),
                            blend_back=0.20):
    """
    Perform ultra-strong denoising in RGB space (prioritizing chroma, then luma), blend two intensity levels using w_far, then blend back slightly with the enhanced image.
    """
    dn = _ultra_denoise_rgb01(enh_rgb01, w_far01, strong=strong, light=light)
    out = (1.0 - blend_back) * dn + blend_back * enh_rgb01
    return np.clip(out, 0, 1).astype(np.float32)

def intensity_gate_mix(orig_rgb01, enh_rgb01, Dnorm01, conf=None, gamma_i=2.0,
                       gate_bias=0.25, gate_floor=0.25, gate_ceil=0.98,
                       anti_vign_sigma=80):
    """
    Intensity Gating: Control brightness enhancement magnitude based on depth. Less enhancement for near (large depth), more for far.
      Original: g0 = (1 - D)^gamma_i * (0.3+0.7*conf)
      Brightness lift: Add bias/floor -> g = clip(g0 + gate_bias, floor, ceil)
      De-vignetting: Remove ultra-low-frequency fluctuations (large-scale Gaussian)
    """
    g0 = (1.0 - np.clip(Dnorm01, 0, 1)) ** float(gamma_i)
    if conf is not None:
        conf = np.clip(conf, 0, 1).astype(np.float32)
        g0 *= (0.3 + 0.7 * conf)

    # —— De-vignetting: Subtract very low-frequency component (large sigma), then return to mean ——
    base = cv.GaussianBlur(g0.astype(np.float32), (0, 0), anti_vign_sigma)
    g = g0 - 0.4 * (base - base.mean())   # 0.4 is adjustable: 0.3~0.5
    # —— Brighten: Bias + Floor/Ceiling ——
    g = g + float(gate_bias)
    g = np.clip(g, float(gate_floor), float(gate_ceil))
    # Light smoothing (avoid blockiness/hard edges)
    g = cv.GaussianBlur(g, (0, 0), 1.0)

    out = (g[..., None] * enh_rgb01 + (1.0 - g[..., None]) * orig_rgb01).astype(np.float32)
    return np.clip(out, 0, 1).astype(np.float32), g.astype(np.float32)



# -------------------- Inference (Single Image) --------------------
def infer_one(image_path: Path,
              net: nn.Module,
              scale_factor: int,
              device: torch.device,
              depth_dir: Path = None,
              use_conf: bool = False,
              denoise_far: bool = False,
              far_gamma: float = 1.0,
              strong_dn=(15, 15, 7, 21),
              light_dn=(0, 0, 7, 21),
              blend_back: float = 0.20,
              # New: Intensity Gating & Depth Dynamic Enhancement Params
              intensity_gate: bool = False,
              gamma_i: float = 2.0,
              d_p1: int = 5,
              d_p2: int = 95,
              d_pow: float = 0.7,
              export_debug: bool = False,
              debug_dir: Path = None,
              args: argparse.Namespace = None,):
    """
    Returns: Enhanced image (np.float32, 0~1) and inference time (s)
    """
    # Read & Preprocess (keep original image for intensity gating)
    im = Image.open(str(image_path)).convert("RGB")
    np_im = np.asarray(im).astype(np.float32) / 255.0  # HWC, RGB
    h, w = np_im.shape[:2]
    h2 = (h // scale_factor) * scale_factor
    w2 = (w // scale_factor) * scale_factor
    np_im = np_im[:h2, :w2, :]

    # to tensor BCHW
    x = torch.from_numpy(np_im).permute(2, 0, 1).unsqueeze(0).to(device).float()

    # Inference
    net.eval()
    if device.type == "cuda":
        torch.cuda.synchronize(device)
    t0 = time.time()
    with torch.no_grad():
        enhanced_image, params_maps = net(x)  # enhanced_image: [1,3,H,W], 0~1
    if device.type == "cuda":
        torch.cuda.synchronize(device)
    dt = time.time() - t0

    # To numpy [H,W,3], 0~1
    enh = enhanced_image.squeeze(0).permute(1, 2, 0).clamp(0, 1).detach().cpu().numpy()

    # If no depth, return enhanced image directly (allow pure baseline)
    if depth_dir is None:
        return enh, dt

    stem = image_path.stem
    D = _load_depth_for_stem(depth_dir, stem, d_p1=d_p1, d_p2=d_p2, d_pow=d_pow)
    if D is None:
        print(f"[WARN] depth not found for {stem}, skip depth-aware.")
        return enh, dt
    conf = _load_conf_for_stem(depth_dir, stem) if use_conf else None
    if conf is not None and conf.shape != D.shape:
        conf = cv.resize(conf, (D.shape[1], D.shape[0]), interpolation=cv.INTER_CUBIC)

    # Unify size to enhanced output size
    H, W = enh.shape[:2]
    if D.shape != (H, W):
        D = cv.resize(D, (W, H), interpolation=cv.INTER_CUBIC)
    if conf is not None and conf.shape != (H, W):
        conf = cv.resize(conf, (W, H), interpolation=cv.INTER_CUBIC)

    # Calculate far-field denoising weight w_far
    w_far = _prep_weight_far(D, conf=conf, target_hw=enh.shape[:2], gamma=far_gamma)

    # Depth-adaptive ultra-strong denoising (RGB)
    if denoise_far:
        enh = depth_aware_denoise_rgb(
            enh_rgb01=enh,
            w_far01=w_far,
            strong=strong_dn,
            light=light_dn,
            blend_back=blend_back
        )

    # Intensity Gating: Use depth to control blending of enhanced vs original (change brightness gain)
    if intensity_gate:
        # —— Intensity Gating (with bias/floor/de-vignetting) ——
        enh, g_far = intensity_gate_mix(
            orig_rgb01=np_im,
            enh_rgb01=enh,
            Dnorm01=D,
            conf=conf,
            gamma_i=gamma_i,
            gate_bias=args.gate_bias,
            gate_floor=args.gate_floor,
            gate_ceil=args.gate_ceil,
            anti_vign_sigma=args.anti_vign_sigma
        )

        # —— Light post-processing: Overall gain + gamma (avoid being too dark), and slight black lift ——
        if args.post_gain != 1.0 or args.post_gamma != 1.0 or args.lift > 0.0:
            enh = np.clip((enh + args.lift) * args.post_gain, 0, 1) ** (1.0 / args.post_gamma)
            enh = np.clip(enh, 0, 1).astype(np.float32)
    else:
        g_far = None

    # Export debug visualization
    if export_debug and debug_dir is not None:
        debug_dir.mkdir(parents=True, exist_ok=True)
        def _save_gray(name, arr01):
            cv.imwrite(str(debug_dir / f"{stem}_{name}.png"),
                       (np.clip(arr01,0,1)*255).astype(np.uint8))
        _save_gray("w_far", w_far)
        if g_far is not None:
            _save_gray("g_far", g_far)
        _save_gray("Dnorm", _norm01(D, 1, 99))
        if conf is not None:
            _save_gray("conf", np.clip(conf,0,1))

    return enh, dt


# -------------------- Main Program --------------------
def main():
    parser = argparse.ArgumentParser(description="Zero-DCE++ with depth-aware postprocessing (ultra noise suppression)")
    parser.add_argument("--input", type=str, help="Input image root directory (can include subdirectories)", default="data/test_data/real")
    parser.add_argument('--dataset_dir', type=str, default=None, help='Optional dataset root (e.g. bdd100k-finetune.v3-bdd100k-night-v3.yolov11)')
    parser.add_argument('--split', type=str, default='test', choices=['test','train','valid','images','real'], help='Dataset split under dataset_dir to use')
    parser.add_argument("--output", type=str, help="Output root directory", default="data/result_Zero_DCE++_depth")
    parser.add_argument("--weights", default="snapshots_Zero_DCE++/Epoch99.pth", type=str, help="Model weights path")
    parser.add_argument("--scale_factor", default=12, type=int, help="scale_factor for Zero-DCE++")
    parser.add_argument("--device", default="0", type=str, help="CUDA device ID (e.g., '0') or 'cpu'")
    # Depth directory
    parser.add_argument("--depth_dir", type=str, default='midas_depth', help="MiDaS depth directory (containing *_Dnorm/disp/conf)")
    parser.add_argument("--use_conf", action="store_true", help="Use *_conf.npy as confidence weight (if exists)")
    # Depth dynamic enhancement (expand dynamic range)
    parser.add_argument("--d_p1", type=int, default=5, help="D percentile lower bound (default 5)")
    parser.add_argument("--d_p2", type=int, default=95, help="D percentile upper bound (default 95)")
    parser.add_argument("--d_pow", type=float, default=0.7, help="D power stretch (<1 enhances near view, default 0.7)")
    # Depth-adaptive denoising parameters
    parser.add_argument("--denoise_far", action="store_true", help="Enable depth-adaptive denoising (stronger for far, lighter for near)")
    parser.add_argument("--far_gamma", type=float, default=2.0, help="Far weight exponent: w_far=(1-D)^gamma (default 2.0)")
    parser.add_argument("--strong", nargs=4, type=int, default=[15, 15, 7, 21],
                        help="Far strong denoising params: h hColor/templateWindow/searchWindow (default 15 15 7 21)")
    parser.add_argument("--light", nargs=4, type=int, default=[0, 0, 7, 21],
                        help="Near light denoising params: h hColor/templateWindow/searchWindow (default 0 0 7 21)")
    parser.add_argument("--blend_back", type=float, default=0.20, help="Blend back ratio to enhanced image (0~1), default 0.20")
    # Intensity Gating (brightness gain controlled by depth)
    parser.add_argument("--intensity_gate", action="store_true", help="Enable intensity gating (change brightness gain to match depth)")
    parser.add_argument("--gamma_i", type=float, default=2.0, help="Intensity gate exponent: g_far=(1-D)^gamma_i (default 2.0)")
    # Others
    parser.add_argument("--exts", nargs="+", default=[".png", ".jpg", ".jpeg", ".bmp"], help="Image extensions to read")
    parser.add_argument("--export_debug", action="store_true", help="Export w_far/g_far/D/conf heatmaps to output/_debug")

    # Intensity Gating brightening / de-vignetting
    parser.add_argument("--gate_bias", type=float, default=0.25, help="Add bias to g_far (brighter overall)")
    parser.add_argument("--gate_floor", type=float, default=0.25, help="Minimum g_far value (avoid blending back to dark original)")
    parser.add_argument("--gate_ceil", type=float, default=0.98, help="Maximum g_far value (avoid overexposure/noise leakage)")
    parser.add_argument("--anti_vign_sigma", type=float, default=80.0, help="De-vignetting: ultra-low-freq smoothing sigma, larger value removes larger rings")

    # Light post-processing (brighter overall but preserve details)
    parser.add_argument("--post_gain", type=float, default=1.15, help="Linear gain (1.0~1.3)")
    parser.add_argument("--post_gamma", type=float, default=0.9, help="Gamma correction (<1 brightens)")
    parser.add_argument("--lift", type=float, default=0.01, help="Lift blacks (prevent blacks from being crushed, which makes it darker)")

    args = parser.parse_args()

    # Device
    if args.device.lower() == "cpu":
        device = torch.device("cpu")
        os.environ.pop("CUDA_VISIBLE_DEVICES", None)
    else:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(args.device)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    cudnn.benchmark = True

    # Model loading (load only once)
    net = model.enhance_net_nopool(args.scale_factor).to(device)
    state = torch.load(args.weights, map_location=device)
    net.load_state_dict(state)

    # Depth directory
    depth_dir = Path(args.depth_dir) if args.depth_dir is not None else None

    # Resolve input images: prefer dataset_dir/<split>/images when provided
    if args.dataset_dir:
        candidate = Path(args.dataset_dir) / args.split / 'images'
        if candidate.exists():
            in_root = candidate
        else:
            candidate2 = Path(args.dataset_dir) / args.split
            in_root = candidate2 if candidate2.exists() else Path(args.input)
    else:
        in_root = Path(args.input)
    out_root = Path(args.output)
    paths = []
    for e in args.exts:
        paths.extend(glob.glob(str(in_root / "**" / f"*{e}"), recursive=True))
    paths = [Path(p) for p in sorted(paths)]
    assert len(paths) > 0, f"No images found under {in_root}"

    # Inference loop
    out_count = 0
    sum_time = 0.0
    dbg_dir = out_root / "_debug" if args.export_debug else None
    for p in paths:
        rel = p.relative_to(in_root)
        out_path = out_root / rel
        out_path.parent.mkdir(parents=True, exist_ok=True)

        enh, dt = infer_one(
            image_path=p,
            net=net,
            scale_factor=args.scale_factor,
            device=device,
            depth_dir=depth_dir,
            use_conf=args.use_conf,
            denoise_far=args.denoise_far,
            far_gamma=args.far_gamma,
            strong_dn=tuple(args.strong),
            light_dn=tuple(args.light),
            blend_back=args.blend_back,
            intensity_gate=args.intensity_gate,
            gamma_i=args.gamma_i,
            d_p1=args.d_p1, d_p2=args.d_p2, d_pow=args.d_pow,
            export_debug=args.export_debug,
            debug_dir=dbg_dir,
            args=args,
        )
        sum_time += dt
        out_count += 1

        # Save (torchvision.utils.save_image accepts tensor)
        enh_t = torch.from_numpy(enh).permute(2, 0, 1).unsqueeze(0).clamp(0, 1).float()
        torchvision.utils.save_image(enh_t, str(out_path))

        print(f"[{out_count}/{len(paths)}] {p}  ->  {out_path}  time={dt:.4f}s")

    print(f"Done. images={out_count}, total_time={sum_time:.3f}s, avg={sum_time/max(out_count,1):.4f}s/img")


if __name__ == "__main__":
    main()